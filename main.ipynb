{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import Set\n",
    "import Utility\n",
    "import SQLSentence\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Set\n",
    "%aimport Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TABLE_CODE = 'stock.stock_code'\n",
    "TABLE_TYPE = 'stock.stock_type'\n",
    "TABLE_CODE_TYPE = 'stock.stock_code_type'\n",
    "\n",
    "PATH_CHECKPOINT = 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Basic Data\n",
    "connect = Utility.connect_to_database()\n",
    "main_data = Utility.GetAllData(connect)\n",
    "list_stock_type = (SQLSentence.QuerySQL(TABLE_CODE_TYPE, connect, ['distinct stock_type_id']))['stock_type_id'].tolist()\n",
    "df_code_name = SQLSentence.QuerySQL(TABLE_CODE, connect)\n",
    "dict_code_name = dict(zip(df_code_name.code, df_code_name.name))\n",
    "dict_type_name = ((SQLSentence.QuerySQL(TABLE_TYPE, connect)).drop(['id'], axis=1)).to_dict()['name']\n",
    "\n",
    "del df_code_name\n",
    "\n",
    "# Build Environment\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if not os.path.exists(PATH_CHECKPOINT):\n",
    "    os.makedirs(PATH_CHECKPOINT)\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, target, window_length_train):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.window_length_train = window_length_train\n",
    "\n",
    "        self.data = self.data.drop(['code', 'date'], axis=1)\n",
    "\n",
    "        self.data = Set.Feature(self.data)\n",
    "\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        self.index_col_open = self.data.columns.get_loc(\"open\")\n",
    "        self.index_col_high = self.data.columns.get_loc(\"high\")\n",
    "        self.index_col_volume = self.data.columns.get_loc(\"volume\") # Get Feature Start Index\n",
    "\n",
    "        self.data = self.data.to_numpy()\n",
    "        self.dim = (self.data.shape[1]-(self.index_col_volume+1))*self.window_length_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.tensor(self.data[index:index+self.window_length_train, self.index_col_volume+1:], dtype=torch.float32).flatten()\n",
    "        target = (self.data[index + self.window_length_train, self.index_col_high]  - \\\n",
    "                  self.data[index + self.window_length_train, self.index_col_open]) / \\\n",
    "                  self.data[index + self.window_length_train, self.index_col_open]\n",
    "        if self.target >= 0:\n",
    "            label = torch.tensor(int(target >= self.target), dtype=torch.float32)\n",
    "        else:\n",
    "            label = torch.tensor(int(target <= self.target), dtype=torch.float32)\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model_1, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*2),\n",
    "            nn.Linear(input_dim*2, input_dim*4),\n",
    "            nn.Linear(input_dim*4, input_dim*8),\n",
    "            nn.Linear(input_dim*8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Functions\n",
    "def ConcatDataSet(main_data, code_list):\n",
    "    all_train_data = []\n",
    "    all_test_data = []\n",
    "\n",
    "    for code in code_list:\n",
    "        stock_data = main_data[main_data['code'] == code].copy()\n",
    "        train_size = int(0.9 * len(stock_data))\n",
    "\n",
    "        all_train_data.append(stock_data.iloc[:train_size])\n",
    "        all_test_data.append(stock_data.iloc[train_size:])\n",
    "\n",
    "    train_data = pd.concat(all_train_data, ignore_index=True)  # 使用 ignore_index=True 来避免重新索引\n",
    "    test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def build_dataloader(main_data, code_list, target, batch_size, window_length_train):\n",
    "    train_data, test_data = ConcatDataSet(main_data, code_list)\n",
    "    dataset_train = StockDataset(train_data, target, window_length_train)\n",
    "    dataset_test = StockDataset(test_data, target, window_length_train)\n",
    "    datalaoder_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "    datalaoder_test = DataLoader(dataset_test, batch_size, shuffle=False)\n",
    "\n",
    "    return datalaoder_train, datalaoder_test\n",
    "\n",
    "def calculate_error_percentage(outputs, label, threshold=0.5):\n",
    "    type1_error = 0\n",
    "    type2_error = 0\n",
    "    type1_count = 0\n",
    "    type2_count = 0\n",
    "\n",
    "    predicted = (outputs.squeeze(1) > threshold).float()\n",
    "\n",
    "    for i in range(label.size(0)):\n",
    "        # Type1\n",
    "        if label[i] == 1:\n",
    "            type1_count += 1\n",
    "            if predicted[i] == 0:\n",
    "                type1_error += 1\n",
    "\n",
    "        # Type2\n",
    "        elif label[i] == 0:\n",
    "            type2_count += 1\n",
    "            if predicted[i] == 1:\n",
    "                type2_error += 1\n",
    "                \n",
    "    type1_error_percentage = round((type1_error/type1_count)*100, 2) if type1_count > 0 else None\n",
    "    type2_error_percentage = round((type2_error/type2_count)*100, 2) if type2_count > 0 else None\n",
    "\n",
    "    return type1_error_percentage, type2_error_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from single Type\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "\n",
    "lr = 1e-4\n",
    "target_pct = 0.02 # Target rise percentage value\n",
    "window_length_train = 20\n",
    "\n",
    "type_id=1\n",
    "\n",
    "list_unique_code_from_data = (SQLSentence.GetCodeByTypeId(type_id, connect))['code'].tolist()\n",
    "datalaoder_train, datalaoder_test = build_dataloader(main_data, list_unique_code_from_data, target_pct, BATCH_SIZE, window_length_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def train_model(model, dataloader, optimizer, criterion, device, threshold):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    accuracy_count = 0\n",
    "    type1_error_percentage_sum = 0\n",
    "    type2_error_percentage_sum = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for input, label in tqdm.tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs.squeeze(1), label)\n",
    "\n",
    "        predicted = (outputs.squeeze(1) > threshold).float()\n",
    "        total_loss+=loss.detach().cpu().item()\n",
    "\n",
    "        correct += (predicted == label).sum().item()\n",
    "        accuracy_count += label.size(0)\n",
    "\n",
    "        type1_error_percentage, type2_error_percentage = calculate_error_percentage(outputs, label, threshold)\n",
    "        if type1_error_percentage is not None:\n",
    "            type1_error_percentage_sum += type1_error_percentage\n",
    "        if type2_error_percentage is not None:\n",
    "            type2_error_percentage_sum += type2_error_percentage\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = round(total_loss/len(dataloader), 6)\n",
    "    accuracy =  round((correct/accuracy_count)*100 , 2)\n",
    "    type1_error_ratio = round(type1_error_percentage_sum/num_batches, 2)\n",
    "    type2_error_ratio = round(type2_error_percentage_sum/num_batches, 2)\n",
    "\n",
    "    return avg_loss, accuracy, type1_error_ratio, type2_error_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model\n",
    "def test_model(model, dataloader, optimizer, criterion, device, threshold):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    accuracy_count = 0\n",
    "    type1_error_percentage_sum = 0\n",
    "    type2_error_percentage_sum = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, label in tqdm.tqdm(dataloader):\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            outputs = model(input)\n",
    "            loss = criterion(outputs.squeeze(1), label)\n",
    "\n",
    "            predicted = (outputs.squeeze(1) > threshold).float()\n",
    "            total_loss+=loss.detach().cpu().item()\n",
    "\n",
    "            correct += (predicted == label).sum().item()\n",
    "            accuracy_count += label.size(0)\n",
    "\n",
    "            type1_error_percentage, type2_error_percentage = calculate_error_percentage(outputs, label, threshold)\n",
    "            if type1_error_percentage is not None:\n",
    "                type1_error_percentage_sum += type1_error_percentage\n",
    "            if type2_error_percentage is not None:\n",
    "                type2_error_percentage_sum += type2_error_percentage\n",
    "\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = round(total_loss/len(dataloader), 6)\n",
    "    accuracy =  round((correct/accuracy_count)*100 , 2)\n",
    "    type1_error_ratio = round(type1_error_percentage_sum/num_batches, 2)\n",
    "    type2_error_ratio = round(type2_error_percentage_sum/num_batches, 2)\n",
    "\n",
    "    return avg_loss, accuracy, type1_error_ratio, type2_error_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select single Type to adjust the model and the optimizer\n",
    "model = Model_1(datalaoder_train.dataset.dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "threshold=0.8\n",
    "best_test_loss = 1000.\n",
    "\n",
    "epoch = 0\n",
    "while epoch < 1:\n",
    "    epoch+=1\n",
    "    print(f'Train {dict_type_name[type_id]} Epoch: {epoch}')\n",
    "\n",
    "    # Train\n",
    "    train_avg_loss, train_accuracy, train_type1_error_ratio, train_type2_error_ratio = train_model(model, datalaoder_train, optimizer, criterion, device, threshold)\n",
    "    \n",
    "    print(f'\\nTrain Loss: {train_avg_loss}')\n",
    "    print(f\"Accuracy: {train_accuracy}%\")\n",
    "    print(f\"Type1 Error Ratio: {train_type1_error_ratio}%\")\n",
    "    print(f\"Type2 Error Ratio: {train_type2_error_ratio}%\\n\")\n",
    "\n",
    "    # Test\n",
    "    test_avg_loss, test_accuracy, test_type1_error_ratio, test_type2_error_ratio = train_model(model, datalaoder_test, optimizer, criterion, device, threshold)\n",
    "    print(f'Test Loss: {test_avg_loss}')\n",
    "    print(f\"Accuracy: {test_accuracy}%\")\n",
    "    print(f\"Type1 Error Ratio: {test_type1_error_ratio}%\")\n",
    "    print(f\"Type2 Error Ratio: {test_type2_error_ratio}%\\n\")\n",
    "    \n",
    "    if test_avg_loss < best_test_loss:\n",
    "        best_test_loss = test_avg_loss\n",
    "        print(f'Model has saved in {PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}\\n')\n",
    "        torch.save(model.state_dict(), f'{PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all data\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "\n",
    "target_pct = 0.02 # Target rise percentage value\n",
    "window_length_train = 20\n",
    "\n",
    "for type_id in list_stock_type:\n",
    "    print(f'Train {dict_type_name[type_id]}')\n",
    "\n",
    "    list_unique_code_from_data = (SQLSentence.GetCodeByTypeId(type_id, connect))['code'].tolist()\n",
    "    train_data, test_data = ConcatDataSet(main_data, list_unique_code_from_data)\n",
    "    datalaoder_train, datalaoder_test = build_dataloader(main_data, list_unique_code_from_data, target_pct, BATCH_SIZE, window_length_train)\n",
    "    \n",
    "    lr = 1e-4\n",
    "    model = Model_1(datalaoder_train.dataset.dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    best_test_loss = 1000.\n",
    "\n",
    "    epoch = 0\n",
    "    while epoch < N_EPOCHS:\n",
    "        epoch+=1\n",
    "\n",
    "        # Train\n",
    "        train_avg_loss, train_accuracy, train_type1_error_ratio, train_type2_error_ratio = train_model(model, datalaoder_train, optimizer, criterion, device, threshold)\n",
    "        \n",
    "        print(f'\\nTrain Loss: {train_avg_loss}')\n",
    "        print(f\"Accuracy: {train_accuracy}%\")\n",
    "        print(f\"Type1 Error Ratio: {train_type1_error_ratio}%\")\n",
    "        print(f\"Type2 Error Ratio: {train_type2_error_ratio}%\\n\")\n",
    "\n",
    "        # Test\n",
    "        test_avg_loss, test_accuracy, test_type1_error_ratio, test_type2_error_ratio = train_model(model, datalaoder_test, optimizer, criterion, device, threshold)\n",
    "        print(f'Test Loss: {test_avg_loss}')\n",
    "        print(f\"Accuracy: {test_accuracy}%\")\n",
    "        print(f\"Type1 Error Ratio: {test_type1_error_ratio}%\")\n",
    "        print(f\"Type2 Error Ratio: {test_type2_error_ratio}%\\n\")\n",
    "                     \n",
    "        if test_avg_loss < best_test_loss:\n",
    "            best_test_loss = test_avg_loss\n",
    "            print(f'Test Loss: {best_test_loss}, Model has saved in {PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')\n",
    "            torch.save(model.state_dict(), f'{PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlstock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
