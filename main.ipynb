{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import Set\n",
    "import Utility\n",
    "import SQLSentence\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Set\n",
    "%aimport Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TABLE_CODE = 'stock.stock_code'\n",
    "TABLE_TYPE = 'stock.stock_type'\n",
    "TABLE_CODE_TYPE = 'stock.stock_code_type'\n",
    "\n",
    "PATH_CHECKPOINT = 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Basic Data\n",
    "connect = Utility.connect_to_database()\n",
    "main_data = Utility.GetAllData(connect)\n",
    "list_stock_type = (SQLSentence.QuerySQL(TABLE_CODE_TYPE, connect, ['distinct stock_type_id']))['stock_type_id'].tolist()\n",
    "df_code_name = SQLSentence.QuerySQL(TABLE_CODE, connect)\n",
    "dict_code_name = dict(zip(df_code_name.code, df_code_name.name))\n",
    "dict_type_name = ((SQLSentence.QuerySQL(TABLE_TYPE, connect)).drop(['id'], axis=1)).to_dict()['name']\n",
    "\n",
    "del df_code_name\n",
    "\n",
    "# Build Environment\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if not os.path.exists(PATH_CHECKPOINT):\n",
    "    os.makedirs(PATH_CHECKPOINT)\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, target, window_length_train):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.window_length_train = window_length_train\n",
    "\n",
    "        self.data = self.data.drop(['code', 'date'], axis=1)\n",
    "\n",
    "        self.data = Set.Feature(self.data)\n",
    "\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        self.index_col_open = self.data.columns.get_loc(\"open\")\n",
    "        self.index_col_high = self.data.columns.get_loc(\"high\")\n",
    "        self.index_col_volume = self.data.columns.get_loc(\"volume\") # Get Feature Start Index\n",
    "\n",
    "        self.data = self.data.to_numpy()\n",
    "        self.dim = (self.data.shape[1]-(self.index_col_volume+1))*self.window_length_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.tensor(self.data[index:index+self.window_length_train, self.index_col_volume+1:], dtype=torch.float32).flatten()\n",
    "        target = (self.data[index + self.window_length_train, self.index_col_high]  - \\\n",
    "                  self.data[index + self.window_length_train, self.index_col_open]) / \\\n",
    "                  self.data[index + self.window_length_train, self.index_col_open]\n",
    "        if self.target >= 0:\n",
    "            label = torch.tensor(int(target >= self.target), dtype=torch.float32)\n",
    "        else:\n",
    "            label = torch.tensor(int(target <= self.target), dtype=torch.float32)\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model_1, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*2),\n",
    "            nn.Linear(input_dim*2, input_dim*4),\n",
    "            nn.Linear(input_dim*4, input_dim*8),\n",
    "            nn.Linear(input_dim*8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Functions\n",
    "def ConcatDataSet(main_data, code_list):\n",
    "    all_train_data = []\n",
    "    all_test_data = []\n",
    "\n",
    "    for code in code_list:\n",
    "        stock_data = main_data[main_data['code'] == code].copy()\n",
    "        train_size = int(0.9 * len(stock_data))\n",
    "\n",
    "        all_train_data.append(stock_data.iloc[:train_size])\n",
    "        all_test_data.append(stock_data.iloc[train_size:])\n",
    "\n",
    "    train_data = pd.concat(all_train_data, ignore_index=True)  # 使用 ignore_index=True 来避免重新索引\n",
    "    test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def build_dataloader(main_data, code_list, target, batch_size, window_length_train):\n",
    "    train_data, test_data = ConcatDataSet(main_data, code_list)\n",
    "    dataset_train = StockDataset(train_data, target, window_length_train)\n",
    "    dataset_test = StockDataset(test_data, target, window_length_train)\n",
    "    datalaoder_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "    datalaoder_test = DataLoader(dataset_test, batch_size, shuffle=False)\n",
    "\n",
    "    return datalaoder_train, datalaoder_test\n",
    "\n",
    "def calculate_error_percentage(outputs, label, threshold=0.5):\n",
    "    type1_error = 0\n",
    "    type2_error = 0\n",
    "    type1_count = 0\n",
    "    type2_count = 0\n",
    "\n",
    "    predicted = (outputs.squeeze(1) > threshold).float()\n",
    "\n",
    "    for i in range(label.size(0)):\n",
    "        # Type1\n",
    "        if label[i] == 1:\n",
    "            type1_count += 1\n",
    "            if predicted[i] == 0:\n",
    "                type1_error += 1\n",
    "\n",
    "        # Type2\n",
    "        elif label[i] == 0:\n",
    "            type2_count += 1\n",
    "            if predicted[i] == 1:\n",
    "                type2_error += 1\n",
    "                \n",
    "    type1_error_percentage = round((type1_error/type1_count)*100, 2) if type1_count > 0 else None\n",
    "    type2_error_percentage = round((type2_error/type2_count)*100, 2) if type2_count > 0 else None\n",
    "\n",
    "    return type1_error_percentage, type2_error_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from single Type\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "\n",
    "lr = 1e-4\n",
    "target_pct = 0.02 # Target rise percentage value\n",
    "window_length_train = 20\n",
    "\n",
    "type_id=1\n",
    "\n",
    "list_unique_code_from_data = (SQLSentence.GetCodeByTypeId(type_id, connect))['code'].tolist()\n",
    "datalaoder_train, datalaoder_test = build_dataloader(main_data, list_unique_code_from_data, target_pct, BATCH_SIZE, window_length_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 紡織 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [01:32<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 24.909193\n",
      "Accuracy: 74.99%\n",
      "Type1 Error Ratio: 99.99%\n",
      "Type2 Error Ratio: 0.02%\n",
      "\n",
      "Test Loss: 12.31049\n",
      "Accuracy: 87.73%\n",
      "Type1 Error Ratio: 97.14%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Model has saved in checkpoint/type_1/type_1_Model_1_Target_0.02\n",
      "\n",
      "Train 紡織 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [01:27<00:00, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 24.937996\n",
      "Accuracy: 75.0%\n",
      "Type1 Error Ratio: 100.0%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Test Loss: 12.310434\n",
      "Accuracy: 87.73%\n",
      "Type1 Error Ratio: 97.14%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Model has saved in checkpoint/type_1/type_1_Model_1_Target_0.02\n",
      "\n",
      "Train 紡織 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [01:26<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 24.938185\n",
      "Accuracy: 75.0%\n",
      "Type1 Error Ratio: 100.0%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Test Loss: 12.309627\n",
      "Accuracy: 87.73%\n",
      "Type1 Error Ratio: 97.14%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Model has saved in checkpoint/type_1/type_1_Model_1_Target_0.02\n",
      "\n",
      "Train 紡織 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [01:35<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 24.972682\n",
      "Accuracy: 74.96%\n",
      "Type1 Error Ratio: 99.89%\n",
      "Type2 Error Ratio: 0.09%\n",
      "\n",
      "Test Loss: 12.341837\n",
      "Accuracy: 87.73%\n",
      "Type1 Error Ratio: 97.14%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Train 紡織 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [01:20<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 24.994447\n",
      "Accuracy: 75.0%\n",
      "Type1 Error Ratio: 100.0%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n",
      "Test Loss: 12.341837\n",
      "Accuracy: 87.73%\n",
      "Type1 Error Ratio: 97.14%\n",
      "Type2 Error Ratio: 0.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select single Type to adjust the model and the optimizer\n",
    "model = Model_1(datalaoder_train.dataset.dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "threshold=0.8\n",
    "best_test_loss = 1000.\n",
    "\n",
    "epoch = 0\n",
    "while epoch < 5:\n",
    "    epoch+=1\n",
    "    print(f'Train {dict_type_name[type_id]} Epoch: {epoch}')\n",
    "\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "    train_total = 0\n",
    "    test_total = 0\n",
    "\n",
    "    #accuracy sum\n",
    "    num_batches = 0\n",
    "    type1_error_percentage_sum = 0\n",
    "    type2_error_percentage_sum = 0\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for input, label in tqdm.tqdm(datalaoder_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs.squeeze(1), label)\n",
    "\n",
    "        predicted = (outputs.squeeze(1) > threshold).float()\n",
    "        train_loss+=loss.detach().cpu().item()\n",
    "\n",
    "        train_correct += (predicted == label).sum().item()\n",
    "        train_total += label.size(0)\n",
    "\n",
    "        type1_error_percentage, type2_error_percentage = calculate_error_percentage(outputs, label, threshold)\n",
    "        if type1_error_percentage is not None:\n",
    "            type1_error_percentage_sum += type1_error_percentage\n",
    "        if type2_error_percentage is not None:\n",
    "            type2_error_percentage_sum += type2_error_percentage\n",
    "        num_batches += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'\\nTrain Loss: {round(train_loss/len(datalaoder_train), 6)}')\n",
    "    print(f\"Accuracy: {round((train_correct/train_total)*100 , 2)}%\")\n",
    "    print(f\"Type1 Error Ratio: {round(type1_error_percentage_sum/num_batches, 2)}%\")\n",
    "    print(f\"Type2 Error Ratio: {round(type2_error_percentage_sum/num_batches, 2)}%\\n\")\n",
    "\n",
    "    #accuracy sum\n",
    "    num_batches = 0\n",
    "    type1_error_percentage_sum = 0\n",
    "    type2_error_percentage_sum = 0\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    for input, label in datalaoder_test:\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        outputs = model(input)\n",
    "        predicted = (outputs.squeeze(1) > 0.5).float()\n",
    "        loss = criterion(outputs.squeeze(1), label)\n",
    "        test_loss+=loss.detach().cpu().item()\n",
    "        test_total += label.size(0)\n",
    "        test_correct += (predicted == label).sum().item()\n",
    "\n",
    "        type1_error_percentage, type2_error_percentage = calculate_error_percentage(outputs, label, threshold)\n",
    "        if type1_error_percentage is not None:\n",
    "            type1_error_percentage_sum += type1_error_percentage\n",
    "        if type2_error_percentage is not None:\n",
    "            type2_error_percentage_sum += type2_error_percentage\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f'Test Loss: {round(test_loss/len(datalaoder_test), 6)}')\n",
    "    print(f\"Accuracy: {round((test_correct/test_total)*100 , 2)}%\")\n",
    "    print(f\"Type1 Error Ratio: {round(type1_error_percentage_sum/num_batches, 2)}%\")\n",
    "    print(f\"Type2 Error Ratio: {round(type2_error_percentage_sum/num_batches, 2)}%\\n\")\n",
    "    \n",
    "    if test_loss/len(datalaoder_test) < best_test_loss:\n",
    "        best_test_loss = test_loss/len(datalaoder_test)\n",
    "        print(f'Model has saved in {PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}\\n')\n",
    "        torch.save(model.state_dict(), f'{PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "\n",
    "target_pct = 0.02 # Target rise percentage value\n",
    "window_length_train = 20\n",
    "\n",
    "for type_id in list_stock_type[1:2]:\n",
    "    print(f'Train {dict_type_name[type_id]}')\n",
    "\n",
    "    list_unique_code_from_data = (SQLSentence.GetCodeByTypeId(type_id, connect))['code'].tolist()\n",
    "    train_data, test_data = ConcatDataSet(main_data, list_unique_code_from_data)\n",
    "    datalaoder_train, datalaoder_test = build_dataloader(main_data, list_unique_code_from_data, target_pct, BATCH_SIZE, window_length_train)\n",
    "    \n",
    "    lr = 1e-4\n",
    "    model = Model_1(datalaoder_train.dataset.dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    best_test_loss = 1000.\n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < N_EPOCHS:\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for input, label in tqdm.tqdm(datalaoder_train):\n",
    "            optimizer.zero_grad()\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            logit = model(input)\n",
    "            loss = criterion(logit.squeeze(1), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.detach().cpu().item()\n",
    "\n",
    "        print(f'Train Loss: {train_loss/len(datalaoder_train)}, Epoch: {epoch+1}')\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        for input, label in datalaoder_test:\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            logit = model(input)\n",
    "            loss = criterion(logit.squeeze(1), label)\n",
    "            test_loss+=loss.detach().cpu().item()\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                print(f'Test Loss: {best_test_loss}, Model has saved in {PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')\n",
    "                torch.save(model.state_dict(), f'{PATH_CHECKPOINT}/type_{type_id}/type_{type_id}_{model.__class__.__name__}_Target_{target_pct}')\n",
    "\n",
    "        epoch+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlstock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
